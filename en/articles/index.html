<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="André Neto's Publications.">
    <meta name="author" content="André Neto">

    <title>André Neto</title>
    <link rel="icon" type="image/png" href="../../favicon.png">
    <link rel="stylesheet" href="../../assets/main.css">

    <script src="../../assets/js/date-manager.js" defer></script>
</head>

<body>
    <header>
        <nav class="navbar container">

            <p><a href="../">André Neto</a></p>

			<p><a href="../about">About me</a></p>
            <h3>Articles</h3>
            <p><a href="../projects/">Projects</a></p>

			<p><a href="../../pt/articles/">Português</a></p>

		</nav>
    </header>

    <main class="container">
        <section class="publication-list">

            <article class="publication-item">
                <h2>Pseudo-MOS Learning: A Hybrid Full-to-No-Reference FIQA Framework</h2>
                <p><strong>Authors:</strong> André Neto, Nuno Gonçalves</p>
                <p><strong>Published in:</strong> <a href="https://www.ibpria.org/2025/" target="_blank"> IbPRIA 2025 &mdash; 12<sup>th</sup> Iberian Conference on Pattern Recognition and Image Analysis </a></p>
                <p><strong>Summary:</strong> This work proposes a hybrid Full-to-No-Reference framework for Facial Image Quality Assessment (FIQA), addressing the gap between traditional IQA metrics and human perception, particularly in steganographically distorted images. The approach begins by training a full-reference fusion metric using classical IQA scores regressed against human MOS. This model generates pseudo-MOS labels across the dataset, enabling the training of a no-reference regressor based on ResNet-18 features. The framework effectively combines full-reference supervision with no-reference inference, providing a scalable, perceptually aligned solution for FIQA.
                    </p>

                <div class="publication-actions">
                    <a href="../../assets/articles/ibpria2025.pdf" class="button" target="_blank" >Read article</a>
                    <a href="../../assets/articles/ibpria2025.pdf" download class="button">Download PDF</a>
                    <a href="../../assets/certificates/ibpria2025_cert.pdf" class="button ml-auto" target="_blank">Certificate</a>
                </div>
            </article>

            <article class="publication-item">
                <h2>Improving Machine Readable Codes Aesthetics: a Novel Iconographic Encoding Standard</h2>
                <p><strong>Authors:</strong> André Neto, Allan Freitas, João Marcos, Nuno Gonçalves</p>
                <p><strong>Published in:</strong> <a href="https://icir.ieee.org/home-2024/" target="_blank"> ICIR 2025 &mdash; IEEE 3<sup>rd</sup> International Conference on Intelligent Reality </a></p>
                <p><strong>Summary:</strong> This paper introduces the novel Icon-based Graphic Code, which is a new paradigm for rachine readable code with configurable aesthetics. Data is encoded as a sequence of icons positioned over the code region. The design and position of the icons is designed for each application, allowing complete control of the final code aesthetics.
                </p>

                <div class="publication-actions">
                    <a href="../../assets/articles/icir2024.pdf" class="button" target="_blank" >Read article</a>
                    <a href="../../assets/articles/icir2024.pdf" download class="button">Download PDF</a>
                    <a href="../../assets/certificates/icir2024_cert.pdf" class="button ml-auto" target="_blank">Certificate</a>
                </div>
            </article>

            <!-- Mais artigos aqui -->
        </section>
    </main>

    <footer class="site-footer container">
        <hr>
        <p>&copy; <span id="copyright"></span> André Neto</p>
        <a href="mailto:email@ags-neto.com">email@ags-neto.com</a>
    </footer>

    <script>
        async function fetchCitations(doi, elementId) {
            try {
                const res = await fetch(`https://api.semanticscholar.org/graph/v1/paper/${doi}?fields=citationCount`);
                const data = await res.json();
                document.getElementById(elementId).textContent = data.citationCount || '0';
            } catch (e) {
                document.getElementById(elementId).textContent = 'N/D';
            }
        }

        fetchCitations('DOI_DO_ARTIGO_AQUI', 'citations-artigo-1');
    </script>
</body>

</html>
